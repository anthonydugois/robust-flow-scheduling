\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[]{todonotes}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}

\title{Scheduling \(K\) classes of jobs with a deterministic oracle}

\begin{document}

\maketitle

\section{Job model}

\begin{itemize}
    \item \(n\) jobs
    \item \(K\) classes of jobs
    \item jobs in class \(k\) have processing time \(p(k)\)
    \item assume that \(p(1)\le p(2)\le\dots\le p(K)\)
    \item no preemption
\end{itemize}

\section{Oracle model}

Assume we do not know in which class a given job is. We consider we have access to a cheap oracle, giving the class of a
job. The oracle is not perfect, i.e., it may give a wrong class for a given job. The oracle is deterministic, i.e., it
always gives the same answer for a given job, no matter if its prediction is correct or not.

We may represent this kind of oracle as a \(K\times K\) matrix \(E\), such that the entry \(e_{ij}\) on the \(i\)-th row
and the \(j\)-th column represents the number of jobs that the oracle believes to be in class \(i\), whereas they are in
reality in class \(j\).

\begin{example}
    Consider the following oracle for \(K=3\) classes of jobs:
    \[
        E=\begin{pmatrix}
            1 & 1 & 3 \\
            2 & 2 & 1 \\
            1 & 1 & 2 \\
        \end{pmatrix}.
    \]
    The oracle believes that:
    \begin{itemize}
        \item 5 jobs are in class 1, among which 1 job is actually in class 2, and 3 jobs are in class 3;
        \item 5 jobs are in class 2, among which 2 jobs are actually in class 1, and 1 job is in class 3;
        \item 4 jobs are in class 3, among which 1 job is actually in class 1, and 1 job is in class 2.
    \end{itemize}
\end{example}

We note \(J_{ij}\) the set of jobs that the oracle believes to be in class \(i\) but are actually in class \(j\) (thus,
\(e_{ij}=|J_{ij}|\)).
We note \(A_k\) (resp.\ \(B_k\)) the set of jobs that are actually in class \(k\) (resp.\ the set of jobs that the
oracle believes to be in class \(k\)), i.e., \(A_k=\bigcup_{i=1}^{K} J_{ik}\) and \(B_k=\bigcup_{j=1}^{K} J_{kj}\).
We also note \(a_k\) (resp.\ \(b_k\)) the number of jobs in \(A_k\) (resp.\ \(B_k\)), i.e.,
\(a_k=|A_k|=\sum_{i=1}^{K} e_{ik}\) and \(b_k=|B_k|=\sum_{j=1}^{K} e_{kj}\).

\subsection{Bloom filter model}

For 2 classes of jobs, the oracle may consist in a probabilistic data structure called a Bloom filter, which represents
a set. Assume a Bloom filter represents the set of \emph{large} jobs. One property of Bloom filters is that they may
produce false positives (i.e., the BF believes that a job is large while it is small), but never produce false
negatives.

Hence, a Bloom filter oracle for 2 classes of jobs can always be represented by a matrix of the following form:
\[
    \begin{pmatrix}
        a & 0 \\
        b & c \\
    \end{pmatrix}.
\]

We may generalize this model by chaining Bloom filters: for \(K\) classes of jobs, use \(K-1\) Bloom filters
\(\mathcal{B}_1,\cdots,\mathcal{B}_{K-1}\), where \(\mathcal{B}_k\) encodes the set of jobs of class \(k\).
As \(K\le 2n\) and querying a Bloom filter takes time \(O(h)\) (where \(h\) is the number of hash functions
used in the filter) with \(h\) much smaller than \(n\), querying the Bloom filter chain remains polynomial.
In this manner, we have an oracle that is guaranteed to never underestimate jobs and can be represented by a lower
triangular matrix.

\section{Knowledge model}

\noindent\textbf{Clairvoyant model.} Everything is known about a problem instance.

\noindent\textbf{Non-clairvoyant model.} Nothing is known about a problem instance.

\noindent\textbf{Oracle-clairvoyant model.} The matrix \(E\) is known.

\noindent\textbf{Realistic BF model.} For an oracle which consists as a chain of BF, we cannot know the full matrix
\(E\) in the general case. Indeed, even if one assumes that the values \(a_1,\cdots,a_K\) are given as the instance,
the structural properties of BF give access only to values \(b_1,\cdots,b_K\) (by a simple application of the oracle)
and \(e_{11},e_{22},\cdots,e_{KK}\) (by deducing the false positive rate of each BF). For example, for a
\(4\times 4\) matrix, we get the following form:
\[
    \begin{pmatrix}
        e_{11} & 0      & 0      & 0      \\
        a      & e_{22} & 0      & 0      \\
        b      & c      & e_{33} & 0      \\
        d      & e      & f      & e_{44} \\
    \end{pmatrix},
\]
where \(a,b,c,d,e,f\) are unknowns. Unfortunately, in the general case one cannot get the knowledge of \(E\) by simply
solving the linear system
\[
    \begin{pmatrix}
        1 & 0 & 0 & 0 & 0 & 0 \\
        0 & 1 & 1 & 0 & 0 & 0 \\
        0 & 0 & 0 & 1 & 1 & 1 \\
        1 & 1 & 0 & 1 & 0 & 0 \\
        0 & 0 & 1 & 0 & 1 & 0 \\
        0 & 0 & 0 & 0 & 0 & 1 \\
    \end{pmatrix}
    \begin{pmatrix}
        a \\ b \\ c \\ d \\ e \\ f \\
    \end{pmatrix}
    =
    \begin{pmatrix}
        b_2-e_{22} \\
        b_3-e_{33} \\
        b_4-e_{44} \\
        a_1-e_{11} \\
        a_2-e_{22} \\
        a_3-e_{33} \\
    \end{pmatrix}.
\]

\todo[inline]{\textbf{TODO}: this seems obvious but maybe can be justified with a variant of Rouch√©-Capelli theorem.}

\section{Minimizing the sum of completion times}

Consider the problem \(1||\sum C_j\).

\subsection{Clairvoyant model}

The optimal algorithm in the clairvoyant model is Shortest Processing Time (SPT), which schedules jobs by non-decreasing order of processing times.

Knowing the exact distribution of job sizes, it is thus quite easy to compute the optimal objective.
Let \(\Delta_{ij}\) be the minimum starting time of jobs in \(J_{ij}\), i.e.,
\[
    \Delta_{ij}=\sum_{i'=1}^{K} \sum_{j'=1}^{j-1} e_{i'j'} \cdot p(j') + \sum_{i'=1}^{i-1} e_{i'j} \cdot p(j).
\]

Hence,
\[
    \text{SPT}=\sum_{i=1}^{K} \sum_{j=1}^{K} e_{ij} \cdot \Delta_{ij} + \frac{e_{ij}(e_{ij}+1)}{2} \cdot p(j).
\]

Then, we can express the objective \(\sum C_j\) of any algorithm \(\mathcal{A}\) as \(\text{SPT}+\sum \eta_j\),
where \(\eta_j\) denotes the additional cost incurred by the job \(j\) in the schedule produced by \(\mathcal{A}\).
In a given schedule, for any job \(j\) in class \(k\), we have
\[
    \eta_j=\sum_{\ell=1}^{k-1} n(\ell,j)\cdot (p(k)-p(\ell)),
\]
where \(n(\ell,j)\) denotes the number of jobs of class \(\ell\) scheduled \emph{after} the job \(j\).

\subsection{Non-clairvoyant model}

In the non-clairvoyant model, simply shuffling jobs yields (in expectation)
\begin{align*}
    \text{RANDOM}&=\mathbb{E}(P_1+(P_1+P_2)+\dots+(P_1+\dots+P_n)) \\
    &=n\mathbb{E}(P_1)+(n-1)\mathbb{E}(P_2)+\dots+\mathbb{E}(P_n) \\
    &=\frac{n(n+1)}{2}\mathbb{E}(P_i) \\
    &=\frac{n+1}{2}\sum p_j,
\end{align*}
where each \(P_i\) is a random variable denoting the processing time of the \(i\)-th scheduled job (we have
\(P(P_i=p(k))=\frac{a_k}{n}\) for any \(i,k\), hence \(\mathbb{E}(P_i)=\sum_{k=1}^{K} \frac{a_k p(k)}{n}\) for any \(i\)).

\subsection{Oracle-clairvoyant model}

Let \(\mathcal{E}(a_1,a_2,\cdots,a_K)\) denote the set of oracles \(E\) such that there are exactly \(a_k\) jobs whose actual class is \(k\), for all \(1\le k\le K\).

In the following, we use the three following notations:
\begin{itemize}
    \item \(W^{\mathcal{A}}_E\left(\sum C_j\right)\): the worst-possible objective given by a (randomized) strategy \(\mathcal{A}\) on an oracle \(E\).
    \item \(\mathbb{E}^{\mathcal{A}}_E\left(\sum C_j\right)\): the expected objective given by a (randomized) strategy \(\mathcal{A}\) on an oracle \(E\).
    \item \(\mathbb{W}^{\mathcal{A}}_{\mathcal{E}(a_1,\cdots,a_K)}\left(\sum C_j\right)=\max_{E\in\mathcal{E}(a_1,\cdots,a_K)} \mathbb{E}^{\mathcal{A}}_E\left(\sum C_j\right)\): the worst-possible expected objective given by a (randomized) strategy \(\mathcal{A}\) (we omit the set \(\mathcal{E}\) when it is clear from the context).
    The oracles \(E\) giving this value are called the \emph{worst-case oracles} for \(\mathcal{A}\).
\end{itemize}
The strategy \(\mathcal{A}\) is omitted when it is clear from the context.
Obviously, our goal is to find a strategy such that \(\mathbb{W}^{\mathcal{A}}\left(\sum C_j\right)<\max_E W^{\mathcal{A}}_E\left(\sum C_j\right)\) (avoiding the worst-possible case in expectation) and \(\mathbb{W}^{\mathcal{A}}\left(\sum C_j\right)\le\rho\cdot\text{SPT}\) for a given \(\rho\ge 1\) (there is a guarantee for any kind of oracle).

Another interesting aspect could be to study which oracles are good for a given strategy, i.e., identifying a subset of \(\mathcal{E}(a_1,\cdots,a_K)\) such that we can obtain a good guarantee for \(\mathbb{W}^{\mathcal{A}}\left(\sum C_j\right)\).

\subsubsection{SPT}

Let us see what is the worst possible objective if we fully trust the oracle, i.e., we try to apply the SPT algorithm with the given predictions.
It can be seen as scheduling the sets \(B_1,B_2,\dots,B_K\) in order, without knowing the ordering of jobs in each \(B_k\).

Thus, in the worst-case, SPT could schedule jobs of each set \(B_k\) in non-increasing order of processing times, yielding for any \(E\)
\[
    W_E\left(\sum C_j\right)=\sum_{i=1}^{K} \sum_{j=1}^{K} e_{ij} \cdot \Delta_{ij} + \frac{e_{ij}(e_{ij}+1)}{2} \cdot p(j),
\]
with
\[
    \Delta_{ij}=\sum_{i'=1}^{i-1} \sum_{j'=1}^{K} e_{i'j'} \cdot p(j') + \sum_{j'=j+1}^{K} e_{ij'} \cdot p(j').
\]

In terms of additional cost compared to the clairvoyant optimal solution, for any job \(\tau\) in the set \(J_{ij}\), we have
\[
    W_E\left(\eta_{\tau}\right)=\sum_{j'=1}^{j-1}\sum_{i'=i}^K e_{i'j'}(p(j)-p(j')),
\]
i.e.,
\[
    W_E\left(\sum\eta_j\right)=\sum_{i=1}^K \sum_{j=1}^K e_{ij} \sum_{j'=1}^{j-1}\sum_{i'=i}^K e_{i'j'}(p(j)-p(j')).
\]

Now, we may wonder what is the expected objective of SPT, assuming that each set \(B_k\) is shuffled independently beforehand in order to avoid putting large jobs first.

Let \(\Delta_i\) denote the minimum starting time of each job in \(B_i\), i.e.,
\[
    \Delta_i=\sum_{i'=1}^{i-1} \sum_{j=1}^{K} e_{i'j} \cdot p(j).
\]

We have \(\mathbb{E}_E(\sum C_j)=\mathbb{E}_E(\sum_{j\in B_1} C_j + \sum_{j\in B_2} C_j + \dots + \sum_{j\in B_K} C_j)=\sum_{i=1}^{K} \mathbb{E}_E(\sum_{j\in B_i} C_j)\), with,
\begin{align*}
    \mathbb{E}_E\left(\sum_{j\in B_i} C_j\right)
    &=b_i \cdot \Delta_i + \mathbb{E}_E(P_1+(P_1+P_2)+\dots+(P_1+\dots+P_{b_i})) \\
    &=b_i \cdot \Delta_i + \frac{b_i(b_i+1)}{2} \cdot \sum_{k=1}^{K} \frac{e_{ik}}{b_i} \cdot p(k) \\
    &=b_i \cdot \Delta_i + \frac{b_i+1}{2} \sum_{j\in B_i} p_j.
\end{align*}

Hence,
\[
    \mathbb{E}_E\left(\sum C_j\right)=\sum_{i=1}^{K} b_i \cdot \Delta_i + \frac{b_i+1}{2} \sum_{j\in B_i} p_j.
\]

The most extreme case is when the oracle is completely wrong, i.e., it is represented by an anti-diagonal matrix.
SPT will schedule jobs in non-increasing order of processing times, yielding the maximum possible objective for the given instance:
\[
    \mathbb{W}\left(\sum C_j\right)=\sum_{i=1}^K e_{i,K-i+1}\cdot\Delta_{i}+\frac{e_{i,K-i+1}(e_{i,K-i+1}+1)}{2}\cdot p(K-i+1),
\]
with
\[
    \Delta_{i}=\sum_{i'=1}^{i-1} e_{i',K-i'+1}\cdot p(K-i'+1),
\]
i.e.,
\[
    \mathbb{W}\left(\sum C_j\right)=\max_E W_E\left(\sum C_j\right).
\]

For any job \(\tau\) in \(J_{i,K-i+1}\), we have
\[
    \mathbb{W}\left(\eta_{\tau}\right)=\sum_{i'=i+1}^K e_{i',K-i'+1}\cdot (p(K-i+1)-p(K-i'+1)),
\]
yielding
\[
    \mathbb{W}\left(\sum \eta_j\right)=\sum_{i=1}^K e_{i,K-i+1}\sum_{i'=i+1}^K e_{i',K-i'+1}\cdot (p(K-i+1)-p(K-i'+1)).
\]

\todo[inline]{\textbf{Question}: which class of oracles yields \(\mathbb{E}^{\text{SPT}}_E\left(\sum C_j\right)\le\text{RANDOM}\)?}

\subsubsection{Picking jobs}

A better algorithm (called PICK) could consist in picking \(e_{i1}\) jobs randomly in each set \(B_i\), and schedule them first.
Then, pick \(e_{i2}\) jobs randomly in each \(B_i\), and schedule them in second. Repeat the procedure until the last column.

There clearly exists an oracle for which the worst-case scenario yields the worst possible objective, i.e., we have
\[
    \max_E W^{\text{PICK}}_E\left(\sum C_j\right)=\max_E W^{\text{SPT}}_E\left(\sum C_j\right).
\]

\todo[inline]{We probably have \(\mathbb{W}^{\text{PICK}}\left(\sum C_j\right)<\max_E W^{\text{PICK}}_E\left(\sum C_j\right)\), i.e., for all oracles we can avoid the worst-case in expectation.}

Let us see what is the expected objective for a given oracle \(E\).
Let \(\tilde{J}_{ij}\) be the set of jobs scheduled during the step \(i+K(j-1)\).
We have
\[
    \mathbb{E}_E\left(\sum C_j\right)=\sum_{i=1}^K \sum_{j=1}^K \mathbb{E}_E\left(\sum_{t\in \tilde{J}_{ij}} C_t\right),
\]
with
\[
    \mathbb{E}_E\left(\sum_{t\in \tilde{J}_{ij}} C_t\right)=
        e_{ij}\mathbb{E}_E\left(\Delta_{ij}\right)+\mathbb{E}_E\left(P_1+(P_1+P_2)+\dots+(P_1+\dots+P_{e_{ij}})\right)
\]
for all \(i,j\), where \(\Delta_{ij}\) is the minimum starting time of jobs in \(\tilde{J}_{ij}\) and each \(P_t\) is a random variable denoting the processing time of the job scheduled at the \(t\)-th position during the current step.

We observe that the algorithm can be seen as shuffling each set \(B_k\) and scheduling the first \(e_{11}\) jobs in \(B_1\), then the first \(e_{21}\) jobs in \(B_2\), and so on.
Thus, we clearly have \(\mathbb{E}_E\left(P_t\right)=\sum_{k=1}^K \frac{e_{ik}}{b_i}\) for any job \(t\) belonging to \(B_i\).
Let \(\bar{p}(i)=\sum_{k=1}^K \frac{e_{ik}}{b_i}\) be the average processing time of jobs \(B_i\).

We also have
\[
    \mathbb{E}_E\left(\Delta_{ij}\right)=
        \sum_{i'=1}^K \sum_{j'=1}^{j-1} e_{i'j'} \bar{p}(i')
        + \sum_{i'=1}^{i-1} e_{i'j} \bar{p}(i'),
\]
which finally yields
\[
    \mathbb{E}_E\left(\sum C_j\right)=
        \sum_{i=1}^K \sum_{j=1}^K\left[
            e_{ij}\left(\sum_{i'=1}^K \sum_{j'=1}^{j-1} e_{i'j'} \bar{p}(i')
                + \sum_{i'=1}^{i-1} e_{i'j} \bar{p}(i')\right) +
            \frac{e_{ij}(e_{ij}+1)}{2}\bar{p}(i)
        \right].
\]

Therefore, if for all \(i,j\), \(e_{ij}=0\) or \(\bar{p}(i)-p(j)\le\varepsilon p(j)\) for a given \(\varepsilon\ge 0\), we have
\[
    \mathbb{E}^{\text{PICK}}_E\left(\sum C_j\right)\le (1+\varepsilon)\text{SPT}.
\]

\todo[inline]{maybe we can be more precise; this does not take into account the fact that \(\bar{p}(i)-p(j)<0\) for some \(i,j\) such that \(e_{ij}>0\).}

\noindent\textbf{Bloom filter model.}
Let us consider instances with 2 classes of jobs, i.e., there are \(a_1\) jobs of class 1 and there are \(a_2\) jobs of class 2.
Consider the Bloom filter model for the oracle, i.e., the oracle is a matrix \(E\) of the following shape:
\[
    E=\begin{pmatrix}
        e_{11} & 0 \\
        e_{21} & e_{22} \\
    \end{pmatrix},
\]
with \(a_1=e_{11}+e_{21}\) and \(a_2=e_{22}\).
In this case, we have
\begin{align*}
    \mathbb{E}_E^{\text{PICK}}\left(\sum C_j\right)
    &=\frac{e_{11}(e_{11}+1)}{2}\bar{p}(1) \\
    &\quad+e_{21}e_{11}\bar{p}(1)+\frac{e_{21}(e_{21}+1)}{2}\bar{p}(2) \\
    &\quad+e_{22}(e_{11}\bar{p}(1)+e_{21}\bar{p}(2))+\frac{e_{22}(e_{22}+1)}{2}\bar{p}(2)
\end{align*}
and
\begin{align*}
    \text{SPT}
    &=\frac{e_{11}(e_{11}+1)}{2}p(1) \\
    &\quad+e_{21}e_{11}p(1)+\frac{e_{21}(e_{21}+1)}{2}p(1) \\
    &\quad+e_{22}(e_{11}p(1)+e_{21}p(1))+\frac{e_{22}(e_{22}+1)}{2}p(2).
\end{align*}

This yields
\begin{align*}
    \mathbb{E}_E^{\text{PICK}}\left(\sum C_j\right)-\text{SPT}
    &=\frac{e_{21}(e_{21}+1)}{2}(\bar{p}(2)-p(1)) \\
    &\quad+e_{22}e_{21}(\bar{p}(2)-p(1)) \\
    &\quad+\frac{e_{22}(e_{22}+1)}{2}(\bar{p}(2)-p(2)).
\end{align*}

As \(\bar{p}(1)=p(1)\) and \(\bar{p}(2)=\frac{e_{21}p(1)+e_{22}p(2)}{e_{21}+e_{22}}\), we have
\[
    \bar{p}(2)-p(1)=\frac{e_{22}}{e_{21}+e_{22}}(p(2)-p(1))
\]
and
\[
    \bar{p}(2)-p(2)=-\frac{e_{21}}{e_{21}+e_{22}}(p(2)-p(1)).
\]

After some calculus, we obtain
\begin{equation}
    \label{eq.diff}
    \mathbb{E}_E^{\text{PICK}}\left(\sum C_j\right)-\text{SPT}=\frac{p(2)-p(1)}{2}e_{21}e_{22}.
\end{equation}

In a Bloom filter, the probability \(\lambda\) of getting a false positive when requesting an element that is not in the set can be approximated as \(\left(1-e^{-kn/m}\right)^k\), where \(n\) is the number of inserted elements, and \(k\) and \(m\) are respectively the number of hash functions and the memory allocated to the Bloom filter.
For a given \(m\), the optimal value for \(k\) (i.e., the value that minimizes \(\lambda\)) is thus \(\frac{m}{n}\ln 2\), which yields
\[
    \lambda\approx\frac{1}{2^{\frac{m}{n}\ln 2}}.
\]
Note that the ratio \(m/n\) represents the average number of bits that are allocated for each element in the Bloom filter.

We can now state the following theorem.

\begin{theorem}
    For any \(\delta>0\), allocating at least
    \[
        \frac{\ln a_1a_2(p(2)-p(1))-\ln 2\delta}{\ln^2 2}
    \]
    bits per job in the Bloom filter implies \(\mathbb{E}_E^{\text{PICK}}\left(\sum C_j\right)-\text{SPT}\le\delta\).
\end{theorem}
\begin{proof}
    Fix \(\delta>0\) and suppose that
    \[
        \frac{m}{a_2}\ge\frac{\ln a_1a_2(p(2)-p(1))-\ln 2\delta}{\ln^2 2}.
    \]

    Then, after some calculus we get
    \[
        \frac{1}{2^{\frac{m}{a_2}\ln 2}}\le\frac{2\delta}{a_1a_2(p(2)-p(1))},
    \]
    that is,
    \[
        \frac{p(2)-p(1)}{2}\lambda a_1a_2\le\delta,
    \]
    where \(\lambda\) is the probability of getting a false positive.

    Observe that \(e_{21}\approx\lambda a_1\) and \(e_{22}=a_2\).
    By Equation~\ref{eq.diff}, we get
    \[
        \mathbb{E}_E^{\text{PICK}}\left(\sum C_j\right)-\text{SPT}\le\delta.
    \]
\end{proof}

\begin{corollary}
    For any \(0\le\alpha<1\), allocating \(\alpha n\) bits in the Bloom filter implies
    \[
        \delta=a_1a_2(p(2)-p(1))\cdot 2^{-\left(\alpha\ln 2 \frac{a_1+a_2}{a_2}+1\right)}.
    \]
\end{corollary}

\newpage
\section{Some algorithms}
\subsection{A DP}
Find the best possible value of $\mathbb{E}(\sum_{j\in\text{Jobs}}Flowtime_j)$. Let be E the state : \[
        E=\begin{pmatrix}
            a_{1\ 1} & 0 & ...& 0 & 0\\
            a_{2\ 1} & a_{2\ 2} & ...& 0 & 0 \\
            ... & ... & ... & ... & ...\\
			a_{m-1\ 1} & a_{m-2\ 2} & ...& a_{m-1\ m-1} & 0 \\	
			a_{m\ 1} & a_{m\ 2} & ... & a_{m\ m-1}& a_{m\ m} \\
        \end{pmatrix}.
    \]

The DP choose a task from non-null line k that minimises g(k), such as : $$
g(k) = \sum_{ i \in [0,m] } \left(\frac{a_{ki}}{\sum_{ j \in [0,m] } a_{kj}}\cdot \left(\sum_{\ell<i}\left(\left(p_i-p_\ell\right)\sum_{q\in[1,m]}a_{q\ell}  \right) + \text{Solve} \begin{pmatrix}
            a_{1\ 1} & 0 & ...& 0 & 0\\
            ... & ... & ... & ... & ...\\
			a_{k1} & ... & a_{ki} -1& ... & 0 \\
			... & ... & ... & ... & ...\\
			a_{m\ 1} & a_{m\ 2} & ... & a_{m\ m-1}& a_{m\ m} \\
        \end{pmatrix}\right)\right)
$$

On an example.

$$
\text{S} \begin{pmatrix}
            0 &   &   \\
            0 & 1 &   \\ 
            4 & 0 & 2 \\
        \end{pmatrix} = \min \begin{cases}
        \text{I can't choose a job from line 1.}\\
        \text{If I choose a job from the line 2, i will pay :}\\
            \hspace{2em}\frac{1}{1}\cdot\left(4\cdot\left(p_2-p_1\right)+ \text{S}\begin{pmatrix}0 &   &   \\0 & 0 &   \\ 4 & 0 & 2 \\ \end{pmatrix}\right) \\ 
        \text{If I choose a job from line 3, i will pay :}\\
            \hspace{2em}\frac{4}{4+2}\left(0+\text{S} \begin{pmatrix}
            0 &   &   \\
            0 & 1 &   \\ 
            3 & 0 & 2 \\
        \end{pmatrix}\right) + \frac{2}{4+2}\left(4\cdot\left(p_3-p_1\right)+1\cdot\left(p_3-p_2\right)+\text{S} \begin{pmatrix}
            0 &   &   \\
            0 & 1 &   \\ 
            4 & 0 & 1 \\
        \end{pmatrix}\right) \\
        \end{cases}
$$
And solving a matrix with no uncertainty gives 0.

\subsubsection*{Complexity}
Let $n$ the number of jobs and $m$ the number of class for a job. The number of different matrices the dynamic program have to compute is $c=\prod_{i\in[1,m]}\prod_{j\in[1,m]}\left(a_{ij}+1\right)$, where $\sum a_{ij}=n$. Nash equilibrium says the biggest value of $c$ is achieved where all values of $a_{ij}$ are equal. The number of relevant values (that are not forced to be equal to zero) in the matrix is $\frac{m(m+1)}{2}$. Since we are in integer domain, we can define an instance where $n$ is divible by $\frac{m(m+1)}{2}$, so all values of $a_{ij}$ are equal. So number of matrices the DP have to compute is near $\frac{n}{0.5m(m+1)}^{0.5m(m+1)}$). This is not polynomial, even if we impose hypothesis like number of class $m$ is bounded by $log(n)$.\\
(We can argue when there is no uncertainty remaining, we can directly say S(matrix)=0, and maybe consider $\frac{1}{2}m(m-1)$ instead of $\frac{1}{2}m(m+1)$ ?)


\subsection{A naive heuristic}
Choose the line with less pondarate average.

Counter example : $p_1 = 1.1$, $p_2 = 3.01$ and $p_3=5.001$.\\
For this matrix $\begin{pmatrix}
            0 &   &   \\
            0 & 1 &   \\ 
            1 & 0 & 1 \\
        \end{pmatrix}$, the average cost of choosing a task from line 2 is $p_2=3.01$, and the average cost from line 3 is $\frac{p_1+p_3}{2}=3.0505$. This heuristic choose line 2 while the DP will find the minimal expected cost with choosing line 3.
 
$$ \text{S}\begin{pmatrix}0 &   &   \\0 & 0 &   \\ 1 & 0 & 1 \\ \end{pmatrix}  =\frac{1}{2}\left(0+\text{S}\begin{pmatrix}0 &   &   \\0 & 0 &  \\ 0 & 0 & 1 \\ \end{pmatrix}\right) + \frac{1}{2}\left((5.001-1.1)+\text{S}\begin{pmatrix}0 &   &   \\0 & 0 &   \\ 0 & 0 & 1 \\ \end{pmatrix}\right) = 1.9505 $$
$$\text{S} \begin{pmatrix}
            0 &   &   \\
            0 & 1 &   \\ 
            1 & 0 & 1 \\
        \end{pmatrix} = \min \begin{cases}
            \frac{1}{1}\cdot\left(\left(3.01-1.1\right)+ \text{S}\begin{pmatrix}0 &   &   \\0 & 0 &   \\ 1 & 0 & 1 \\ \end{pmatrix}\right)  = 3.8605 \\
            \frac{1}{2}\left(0+\text{S} \begin{pmatrix}
            0 &   &   \\
            0 & 1 &   \\ 
            0 & 0 & 1 \\
        \end{pmatrix}\right) + \frac{1}{2}\left(\left(5.001-1.1\right)+\left(5.001-3.01\right)+\text{S} \begin{pmatrix}
            0 &   &   \\
            0 & 1 &   \\ 
            1 & 0 & 0 \\
        \end{pmatrix}\right)=2.946 \\
        \end{cases}
$$


 
 
\end{document}
